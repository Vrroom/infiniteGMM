{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infinite Gaussian Mixture Model\n",
    "\n",
    "The paper presents a Markov Chain Monte Carlo method for inference based on an Infinite Gaussian Mixture Model. The data is assumed to have been generated from a countably infinite Mixture of Gaussians. The MCMC method gives us the parameters of the Gaussian components that generated the data. One might wonder how the method can possibly tell us infinitely many parameters. This is not a concern because at any time, we'll see only finite amount of data. The method will give us only the relevant parameters.\n",
    "\n",
    "This method is applicable when one wants to cluster data without knowing the number of clusters in advance. In this implementation, the observations will be one-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "from sampler import *\n",
    "import pprint\n",
    "\n",
    "# Generate test data.\n",
    "y = np.concatenate([np.random.normal(-20, 1, 500),\n",
    "                    np.random.normal(0, 1, 3000),\n",
    "                    np.random.normal(20, 1, 1000)]).reshape(-1, 1)\n",
    "mu_y = np.mean(y)\n",
    "var_y = np.var(y)\n",
    "\n",
    "c = np.array([0]*500 + [1]*3000 + [2]*1000)\n",
    "n = len(c)\n",
    "plt.hist(y, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that $k$, the number of mixture components was known. The model parameters would be $k$ means and $k$ precisions for the Gaussians, and the mixture weights. \n",
    "\n",
    "Means are denoted by $\\mu_1, \\cdots, \\mu_k$. Precisions by $s_1, \\cdots, s_k$. Finally, the mixture weights are denoted by $\\pi_1, \\cdots, \\pi_k$.\n",
    "\n",
    "\\begin{equation}\n",
    "p(y | \\mu_1, \\cdots, \\mu_k, s_1, \\cdots, s_k, \\pi_1, \\cdots, \\pi_k) = \\sum_{i = 1}^k \\pi_i \\mathcal{N}(\\mu_i, \\frac{1}{s_i})\n",
    "\\end{equation} \n",
    "\n",
    "The model parameters have hyperparameters associated with them. Parameters $\\mu_i$ are controlled by two hyperparameters $\\lambda$ and $r$. Parameters $s_i$ are controlled by $\\beta$ and $\\omega$. The mixture weights are controlled by a single hyperparameter $\\alpha$. \n",
    "\n",
    "The Bayes Net, with the priors on the hyperparameters is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"566pt\" height=\"458pt\"\n",
       " viewBox=\"0.00 0.00 566.27 457.81\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 453.8133)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-453.8133 562.267,-453.8133 562.267,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M166.1335,-232.9066C166.1335,-232.9066 457.1335,-232.9066 457.1335,-232.9066 463.1335,-232.9066 469.1335,-238.9066 469.1335,-244.9066 469.1335,-244.9066 469.1335,-334.86 469.1335,-334.86 469.1335,-340.86 463.1335,-346.86 457.1335,-346.86 457.1335,-346.86 166.1335,-346.86 166.1335,-346.86 160.1335,-346.86 154.1335,-340.86 154.1335,-334.86 154.1335,-334.86 154.1335,-244.9066 154.1335,-244.9066 154.1335,-238.9066 160.1335,-232.9066 166.1335,-232.9066\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.1335\" y=\"-240.7066\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster4,500 x 1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M270.1335,-8C270.1335,-8 346.1335,-8 346.1335,-8 352.1335,-8 358.1335,-14 358.1335,-20 358.1335,-20 358.1335,-109.9533 358.1335,-109.9533 358.1335,-115.9533 352.1335,-121.9533 346.1335,-121.9533 346.1335,-121.9533 270.1335,-121.9533 270.1335,-121.9533 264.1335,-121.9533 258.1335,-115.9533 258.1335,-109.9533 258.1335,-109.9533 258.1335,-20 258.1335,-20 258.1335,-14 264.1335,-8 270.1335,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"323.6335\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4,500 x 1</text>\n",
       "</g>\n",
       "<!-- lambda -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>lambda</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"308.1335\" cy=\"-412.3366\" rx=\"41.9398\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"308.1335\" y=\"-423.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">lambda</text>\n",
       "<text text-anchor=\"middle\" x=\"308.1335\" y=\"-408.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"308.1335\" y=\"-393.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Normal</text>\n",
       "</g>\n",
       "<!-- mu -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>mu</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"308.1335\" cy=\"-301.3833\" rx=\"41.9398\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"308.1335\" y=\"-312.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mu</text>\n",
       "<text text-anchor=\"middle\" x=\"308.1335\" y=\"-297.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"308.1335\" y=\"-282.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Normal</text>\n",
       "</g>\n",
       "<!-- lambda&#45;&gt;mu -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>lambda&#45;&gt;mu</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M308.1335,-374.7729C308.1335,-366.549 308.1335,-357.7367 308.1335,-349.1736\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"311.6336,-348.9083 308.1335,-338.9083 304.6336,-348.9084 311.6336,-348.9083\"/>\n",
       "</g>\n",
       "<!-- omega -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>omega</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"43.1335\" cy=\"-412.3366\" rx=\"43.2674\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"43.1335\" y=\"-423.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">omega</text>\n",
       "<text text-anchor=\"middle\" x=\"43.1335\" y=\"-408.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"43.1335\" y=\"-393.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Gamma</text>\n",
       "</g>\n",
       "<!-- s -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>s</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"205.1335\" cy=\"-301.3833\" rx=\"43.2674\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"205.1335\" y=\"-312.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">s</text>\n",
       "<text text-anchor=\"middle\" x=\"205.1335\" y=\"-297.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"205.1335\" y=\"-282.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Gamma</text>\n",
       "</g>\n",
       "<!-- omega&#45;&gt;s -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>omega&#45;&gt;s</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M76.2966,-388.1684C82.5274,-383.7038 89.0083,-379.1109 95.1335,-374.86 116.9252,-359.7363 141.4445,-343.3027 161.7943,-329.8198\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"163.8297,-332.6699 170.2402,-324.2345 159.9685,-326.8311 163.8297,-332.6699\"/>\n",
       "</g>\n",
       "<!-- beta -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>beta</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"176.1335\" cy=\"-412.3366\" rx=\"72.25\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"176.1335\" y=\"-423.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">beta</text>\n",
       "<text text-anchor=\"middle\" x=\"176.1335\" y=\"-408.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"176.1335\" y=\"-393.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">InverseGamma</text>\n",
       "</g>\n",
       "<!-- beta&#45;&gt;s -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>beta&#45;&gt;s</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M185.8708,-375.0821C188.1667,-366.2979 190.6419,-356.828 193.0281,-347.6985\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"196.4271,-348.5343 195.5697,-337.9742 189.6546,-346.7641 196.4271,-348.5343\"/>\n",
       "</g>\n",
       "<!-- r -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>r</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"411.1335\" cy=\"-412.3366\" rx=\"43.2674\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"411.1335\" y=\"-423.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">r</text>\n",
       "<text text-anchor=\"middle\" x=\"411.1335\" y=\"-408.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"411.1335\" y=\"-393.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Gamma</text>\n",
       "</g>\n",
       "<!-- r&#45;&gt;mu -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>r&#45;&gt;mu</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M383.8018,-382.8945C370.9109,-369.0082 355.4073,-352.3074 341.7991,-337.6484\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"344.2481,-335.1422 334.8795,-330.1945 339.1179,-339.9046 344.2481,-335.1422\"/>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"403.1335\" cy=\"-187.43\" rx=\"57.0522\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"403.1335\" y=\"-198.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">c</text>\n",
       "<text text-anchor=\"middle\" x=\"403.1335\" y=\"-183.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"403.1335\" y=\"-168.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Categorical</text>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"#d3d3d3\" stroke=\"#000000\" cx=\"308.1335\" cy=\"-76.4767\" rx=\"41.9398\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"308.1335\" y=\"-87.7767\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "<text text-anchor=\"middle\" x=\"308.1335\" y=\"-72.7767\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"308.1335\" y=\"-57.7767\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Normal</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;y -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>c&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M374.8876,-154.4407C364.1077,-141.8506 351.7352,-127.4004 340.5937,-114.3879\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"343.0034,-111.8208 333.8409,-106.5011 337.6862,-116.3735 343.0034,-111.8208\"/>\n",
       "</g>\n",
       "<!-- alpha -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>alpha</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"515.1335\" cy=\"-412.3366\" rx=\"43.2674\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"515.1335\" y=\"-423.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">alpha</text>\n",
       "<text text-anchor=\"middle\" x=\"515.1335\" y=\"-408.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"515.1335\" y=\"-393.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Gamma</text>\n",
       "</g>\n",
       "<!-- pi -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>pi</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"414.1335\" cy=\"-301.3833\" rx=\"46.8387\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"414.1335\" y=\"-312.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">pi</text>\n",
       "<text text-anchor=\"middle\" x=\"414.1335\" y=\"-297.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"414.1335\" y=\"-282.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Dirichlet</text>\n",
       "</g>\n",
       "<!-- alpha&#45;&gt;pi -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>alpha&#45;&gt;pi</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M488.3325,-382.8945C476.2024,-369.5689 461.713,-353.6516 448.7707,-339.4339\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"451.0844,-336.7762 441.7645,-331.7373 445.9079,-341.4884 451.0844,-336.7762\"/>\n",
       "</g>\n",
       "<!-- mu&#45;&gt;y -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>mu&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M308.1335,-263.739C308.1335,-225.466 308.1335,-166.0344 308.1335,-124.417\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"311.6336,-124.2438 308.1335,-114.2438 304.6336,-124.2438 311.6336,-124.2438\"/>\n",
       "</g>\n",
       "<!-- s&#45;&gt;y -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>s&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M221.2273,-266.2416C239.2169,-226.9601 268.3782,-163.2848 287.8983,-120.6615\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"291.1711,-121.9208 292.1528,-111.3716 284.8067,-119.0061 291.1711,-121.9208\"/>\n",
       "</g>\n",
       "<!-- pi&#45;&gt;c -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>pi&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M410.5012,-263.7549C409.6188,-254.6141 408.6637,-244.7194 407.7428,-235.1793\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"411.2067,-234.6363 406.762,-225.0189 404.2391,-235.3089 411.2067,-234.6363\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x101b59e80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "with pm.Model() as finiteGMMmodel:\n",
    "    # TODO: Distribution parameters may be wrong here.\n",
    "    lambda_ = pm.Normal('lambda', mu=mu_y, tau=1/var_y)\n",
    "    r_ = pm.Gamma('r', mu=1, sigma=1/var_y)\n",
    "    mu_ = pm.Normal('mu', mu=lambda_, tau=r_, shape=k)\n",
    "    beta_ = pm.InverseGamma('beta', mu=1, sigma=1)\n",
    "    omega_ = pm.Gamma('omega', alpha=1, beta=var_y)\n",
    "    s_ = pm.Gamma('s', mu=beta_, sigma=1/omega_, shape=k)\n",
    "    alpha_ = pm.Gamma('alpha', alpha=1, beta=1)\n",
    "    pi_ = pm.Dirichlet('pi', a=np.ones(k) * (alpha_ / k))\n",
    "    c_ = pm.Categorical('c', p=pi_)\n",
    "    y_ = pm.Normal('y', mu=mu_[c_], tau=s_[c_], observed=y)\n",
    "\n",
    "pm.model_to_graphviz(finiteGMMmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having observed data, the parameters and the hyperparameters can be inferred by Gibbs Sampling.\n",
    "\n",
    "In the Gibbs Sampling update, the (hyper)parameters are listed in a sequence and each (hyper)parameter is updated by sampling from its distribution conditioned on all the other (hyper)parameters. Repeated application of this update ensures that in the limit, the values of the (hyper)parameters obtained arise from the joint distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\vec{\\mu}, \\vec{s}, \\vec{\\pi}, c_1, \\cdots, c_n, \\lambda, r, \\beta, \\omega, \\alpha |y_1, \\cdots, y_n)\n",
    "\\end{equation}\n",
    "\n",
    "To cover my bases, I'll assume that the $c_i$'s are known. I'll use the Gibbs Sampling updates to infer model parameters from the following distribution.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\vec{\\mu}, \\vec{s}, \\vec{\\pi}, \\lambda, r, \\beta, \\omega, \\alpha |c_1, \\cdots, c_n, y_1, \\cdots, y_n)\n",
    "\\end{equation}\n",
    "\n",
    "The conditional for the means are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\mu_j | \\vec{c}, \\vec{y}, s_j, \\lambda, r) \\sim \\mathcal{N}(\\frac{\\bar{y_j}n_js_j + \\lambda r}{n_js_j+r}, \\frac{1}{n_js_j + r})\n",
    "\\end{equation}\n",
    "\n",
    "Where, $\\bar{y_j}$ is sum of all those y's in class $j$. We compute the conditional for the means of all classes simultaneously over here. Note that only variables upon which $\\mu_j$ is conditioned on contain it's Markov Blanket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSampler (c, y, s, lambda_, r, k, **kwargs) :\n",
    "    n = np.array([np.sum(c == j) for j in range(k)])\n",
    "    y_ = np.array([np.sum(y[c == j]) / nj for j, nj in enumerate(n)])\n",
    "    mean = (y_ * n * s + lambda_ * r) / (n * s + r)\n",
    "    var = 1 / (n * s + r)\n",
    "    sample = rng.randn(k)\n",
    "    return mean + np.sqrt(var) * sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional for the precisions are: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(s_j | \\vec{c}, \\vec{y}, \\mu_j, \\beta, \\omega) \\sim \\mathcal{G}(\\beta + n_j, \\Big(\\frac{1}{\\beta + n_j}(\\omega\\beta + \\sum_{i:c_i=j}(y_i - \\mu_j)^2)\\Big)^{-1})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisionSampler (c, y, mu, beta, omega, k, **kwargs) :\n",
    "    n = np.array([np.sum(c == j) for j in range(k)])\n",
    "    deltas = np.array([np.sum(np.linalg.norm(y[c == j] - mu_j)**2) for j, mu_j in enumerate(mu)])\n",
    "    shape = beta + n\n",
    "    mean = ((deltas + omega * beta) / shape) ** (-1)\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional for $\\lambda$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\lambda | \\vec{\\mu}, r) \\sim \\mathcal{N}(\\frac{\\mu_y\\sigma_y^{-2}+r\\sum_{j=1}^k\\mu_j}{\\sigma_y^{-2}+kr}, \\frac{1}{\\sigma_y^{-2}+kr})\n",
    "\\end{equation}\n",
    "\n",
    "Conditional for $r$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(r | \\vec{\\mu}, \\lambda) \\sim \\mathcal{G}(k+1, \\Big(\\frac{1}{k+1}(\\sigma_y^2 + \\sum_{j=1}^k(\\mu_j - \\lambda)^2)\\Big)^{-1})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaSampler(mu, mu_y, var_y, k, r, **kwargs) :\n",
    "    mean = (mu_y*1/var_y + r*sum(mu))/(1/var_y + k*r)\n",
    "    var = 1/(1/var_y + k*r)\n",
    "    return mean + np.sqrt(var) * rng.randn()\n",
    "\n",
    "def rSampler (mu, mu_y, var_y, k, lambda_, **kwargs) :\n",
    "    shape = k + 1\n",
    "    mean = ((1/shape)*(var_y + np.linalg.norm(mu - lambda_)**2))**-1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional for $\\beta$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\beta | \\vec{s}, \\omega) \\propto \\Gamma(\\frac{\\beta}{2})^{-k}exp(\\frac{-1}{2\\beta})(\\frac{\\beta}{2})^{(k\\beta-3)/2}\\prod_{j=1}^k(s_j\\omega)^{\\beta/2}exp(-\\frac{\\beta s_j \\omega}{2})\n",
    "\\end{equation}\n",
    "\n",
    "We'll use Adaptive Rejection Sampling to sample from $\\beta$'s. I'm confused because if we need just one sample, what is the point of using ARS. I need to check whether ARS works when given an unnormalized distribution. Maybe it is hard to get good proposals in general.\n",
    "\n",
    "Conditional for $\\omega$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\omega | \\vec{s}, \\beta) \\sim \\mathcal{G}(k\\beta+1, \\Big(\\frac{1}{k\\beta+1}(\\sigma_y^2 + \\beta\\sum_{j=1}^ks_j)\\Big)^{-1})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaSampler (s, omega, k, **kwargs) :\n",
    "    def logU (y) :\n",
    "        beta = torch.exp(y)\n",
    "        t1 = (-k * torch.lgamma(beta/2))\n",
    "        t2 = (-1/(2*beta))\n",
    "        t3 = ((k*beta-3)/2)*(y - math.log(2))\n",
    "        t4 = (beta/2)*np.sum(np.log(s*omega) - s*omega)\n",
    "        return y + t1 + t2 + t3 + t4\n",
    "    sample = adaptiveRejectionSampling(logU, 1, (-math.inf, math.inf))\n",
    "    return np.exp(sample)\n",
    "\n",
    "def omegaSampler (s, var_y, k, beta, **kwargs) :\n",
    "    shape = k*beta + 1\n",
    "    mean = ((1/shape)*(var_y + beta * sum(s)))**-1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined all the conditionals, we can do inference using Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 42.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'beta': array([0.3231816], dtype=float32),\n",
      "  'lambda_': 0.9692207017850285,\n",
      "  'mu': array([-1.99508864e+01, -4.10546271e-03,  1.99973006e+01]),\n",
      "  'omega': array([0.03061173]),\n",
      "  'r': 0.004589499154456601,\n",
      "  's': array([0.90014372, 1.05316527, 1.06590664])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'mu': np.zeros(3),\n",
    "    's': np.ones(3),\n",
    "    'lambda_': mu_y,\n",
    "    'r': 1, \n",
    "    'beta': var_y,\n",
    "    'omega': var_y\n",
    "}\n",
    "\n",
    "samplers = {\n",
    "    'mu': meanSampler,\n",
    "    's': precisionSampler,\n",
    "    'lambda_': lambdaSampler,\n",
    "    'r': rSampler, \n",
    "    'beta': betaSampler,\n",
    "    'omega': omegaSampler\n",
    "}\n",
    "\n",
    "others = {\n",
    "    'k': 3,\n",
    "    'mu_y': mu_y,\n",
    "    'var_y': var_y,\n",
    "    'c': c,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "inferredParameters = gibbs(parameters, samplers, 10, **others)\n",
    "pprint.PrettyPrinter(indent=2).pprint(inferredParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task was easy. Since we knew the components, the problem became analogous to fitting three independent gaussians instead of a mixture. Also, the data, once seperated on the basis of class, gave good indication of the mean and the variance for the mixture components.\n",
    "\n",
    "Now, let's suppose that the $c_i$'s are not known. Firstly, we integrate over all $\\vec{\\pi}$ so that parent of the $c_i$'s is $\\alpha$. Then, the conditional for $\\alpha$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\alpha | \\vec{c}) \\propto \\frac{\\alpha^{k - 3/2}exp(\\frac{-1}{2\\alpha})\\Gamma(\\alpha)}{\\Gamma(n + \\alpha)}\n",
    "\\end{equation}\n",
    "\n",
    "We'll use Adaptive Rejection Sampling to obtain samples of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaSampler (c, k, **kwargs) : \n",
    "    n = len(c)\n",
    "    def logU (y) : \n",
    "        alpha = torch.exp(y)\n",
    "        t1 = y * (k - (3/2))\n",
    "        t2 = -1/(2 * alpha)\n",
    "        t3 = torch.lgamma(alpha)\n",
    "        t4 = -torch.lgamma(n + alpha)\n",
    "        return y + t1 + t2 + t3 + t4\n",
    "    sample = adaptiveRejectionSampling(logU, 1, (-math.inf, math.inf))\n",
    "    return np.exp(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov Blanket of $c_i$ contains $y_i$, $\\vec{\\mu}$, $\\vec{s}$ and $\\alpha$. Hence, we have to find an expression for:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(c_i | y_i, \\vec{\\mu}, \\vec{s}, \\alpha) \n",
    "\\end{equation}\n",
    "\n",
    "Each $c_i$ is conditionally independent of the $\\vec{c}_{-i}$. Hence, if we condition on this variable as well, it doesn't matter.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(c_i | y_i, \\vec{\\mu}, \\vec{s}, \\alpha) = \\mathbf{Pr}(c_i | y_i, \\vec{\\mu}, \\vec{s}, \\alpha, \\vec{c}_{-i}) \n",
    "\\end{equation}\n",
    "\n",
    "Now, it is much simpler to factor the conditional probability distribution.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(c_i | y_i, \\vec{\\mu}, \\vec{s}, \\alpha, \\vec{c}_{-i}) \\propto \\mathbf{Pr}(y_i | \\vec{c}, \\vec{\\mu}, \\vec{s}, \\alpha) \\times \\mathbf{Pr}(c_i | \\vec{c}_{-i}, \\vec{\\mu}, \\vec{s}, \\alpha)\n",
    "\\end{equation}\n",
    "\n",
    "The first term is simply a Gaussian centered at $\\mu_{c_i}$ with precision $s_{c_i}$. Since, $c_i$ is conditionally independent of all non-descendants given it's parent $\\alpha$, the second term is becomes $\\mathbf{Pr}(c_i | \\vec{c}_{-i}, \\alpha)$. This has a closed form solution obtained by solving the Dirichlet Integral to integrate out $\\vec{\\pi}$. \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(c_i | \\vec{c}_{-i}, \\alpha) = \\frac{n_{-i, j} + \\frac{\\alpha}{k}}{n - 1 + \\alpha}\n",
    "\\end{equation}\n",
    "\n",
    "Here, $n_{-i, j}$ is the number of $j$ components when data point $i$ is not included.\n",
    "\n",
    "Although according to Gibbs Sampling, we have to update the $c_i$'s sequentially, I'll update them in parallel because I don't think it makes that big a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cSampler (c, y, mu, s, alpha, k, **kwargs) :\n",
    "    n = len(c)\n",
    "    nj = np.array([np.sum(c == j) for j in range(k)])\n",
    "    nij = k\n",
    "    delta = (np.repeat(y, k, axis=1) - mu) ** 2\n",
    "    probs = ((nij + (alpha / k)) / (n - 1 + alpha)) * (s**(1/2) * np.exp(-s * delta/ 2))\n",
    "    probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "    return np.array([np.argmax(rng.multinomial(1, pvals=pvals)) for pvals in probs])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:51<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'alpha': array([0.28975776], dtype=float32),\n",
      "  'beta': array([0.47218418], dtype=float32),\n",
      "  'c': array([1, 1, 1, ..., 2, 2, 2]),\n",
      "  'lambda_': -18.414350389512265,\n",
      "  'mu': array([ -0.03428457, -19.95607765,  20.02396498]),\n",
      "  'omega': array([0.00491288]),\n",
      "  'r': 0.0016431592232967359,\n",
      "  's': array([1.05276387, 0.9109834 , 1.0449161 ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'alpha': 1,\n",
    "    'mu': np.zeros(3),\n",
    "    's': np.ones(3),\n",
    "    'lambda_': mu_y,\n",
    "    'r': 1, \n",
    "    'beta': var_y,\n",
    "    'omega': var_y,\n",
    "    'c': np.array([np.random.randint(0, k) for _ in range(n)])\n",
    "}\n",
    "\n",
    "samplers = {\n",
    "    'alpha': alphaSampler,\n",
    "    'c': cSampler,\n",
    "    'mu': meanSampler,\n",
    "    's': precisionSampler,\n",
    "    'lambda_': lambdaSampler,\n",
    "    'r': rSampler, \n",
    "    'beta': betaSampler,\n",
    "    'omega': omegaSampler\n",
    "}\n",
    "\n",
    "others = {\n",
    "    'k': 3,\n",
    "    'mu_y': mu_y,\n",
    "    'var_y': var_y,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "inferredParameters = gibbs(parameters, samplers, 200, **others)\n",
    "pprint.PrettyPrinter(indent=2).pprint(inferredParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the means and the precisions right! Now, we need to take it to the infinite limit. The extension is quite straightforward to implement but a bit difficult to derive. \n",
    "\n",
    "In the beginning, none of the data points $y_i$ will have a component associated with them. They will be associated with a pseudo-component corresponding to all the unrepresented components. \n",
    "\n",
    "During the sampling of the indicator variables, for each $y_i$, it'll be decided whether the corresponding component is one of the finite components made so far or whether the component is one of the infinite unrepresented components. If an unrepresented component is chosen for the $y_i$, then a new class label is added and the mean and precision for this new class label is added. \n",
    "\n",
    "At any point, if there is a component which has no $y_i$ associated with it, then that component, along with it's parameters is discarded.\n",
    "\n",
    "In this manner, the algorithm adaptively chooses $k$, the correct number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "def unrepresentedMeanSampler (lambda_, r, **kwargs) : \n",
    "    var = 1 / r\n",
    "    return lambda_ + np.sqrt(var) * rng.randn()\n",
    "\n",
    "def unrepresentedPrecisionSampler (beta, omega, **kwargs) :\n",
    "    shape = beta\n",
    "    mean = 1 / omega\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)\n",
    "\n",
    "def cSampler (c, y, mu, s, unrepresentedMu, unrepresentedPrecision, alpha, lambda_, r, beta, omega, **kwargs) :\n",
    "    global k\n",
    "    s_, mu_ = np.array(s), np.array(mu)\n",
    "    n = len(c)\n",
    "    nij = (n - np.array([c == j for j in range(k)])).T\n",
    "    delta1 = (np.repeat(y, k, axis=1) - mu) ** 2\n",
    "    delta2 = (y - unrepresentedMu) ** 2  \n",
    "    probs1 = (nij / (n - 1 + alpha)) * (np.sqrt(s_/(2*np.pi)) * np.exp(-s_ * delta1 / 2))\n",
    "    likelihood = np.sqrt(unrepresentedPrecision) * np.exp(-unrepresentedPrecision * delta2 / 2) \n",
    "    probs2 = (alpha / (n - 1 + alpha)) * likelihood\n",
    "    # Probabilities of the existing components concatenated with probability\n",
    "    # for all the other components put together.\n",
    "    probs = np.concatenate((probs1, probs2), axis=1)\n",
    "    probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "    # New Components sampled using those probabilities\n",
    "    newC = np.array([np.argmax(rng.multinomial(1, pvals=pvals)) for pvals in probs])\n",
    "    new_nj = np.array([np.sum(c == j) for j in range(k)])\n",
    "    # Time to remove all those components who have no \n",
    "    # data point associated with them anymore!\n",
    "    notClassLabels = np.array(range(k))[new_nj == 0]\n",
    "    for label in notClassLabels : \n",
    "        newC[newC == label] = -1\n",
    "    # Re-index the remaining component labels.\n",
    "    classLabels = np.array(range(k))[new_nj > 0]\n",
    "    mu.clear()\n",
    "    mu.extend(mu_[new_nj > 0].tolist())\n",
    "    s.clear()\n",
    "    s.extend(s_[new_nj > 0].tolist())\n",
    "    newK = sum(new_nj > 0)\n",
    "    for i, label in enumerate(classLabels) :\n",
    "        newC[newC == label] = i\n",
    "    # If any data point is assigned one of the \n",
    "    # unrepresented classes, initialize a new class\n",
    "    # with parameters mean and precision. \n",
    "    if sum(newC == k) > 0 : \n",
    "        newC[newC == k] = newK\n",
    "        mu.append(unrepresentedMu)\n",
    "        s.append(unrepresentedPrecision)\n",
    "        k = newK + 1\n",
    "    return newC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "## Redefining the previous samplers so that they can\n",
    "## work with varying number of components.\n",
    "######################################################\n",
    "\n",
    "def meanSampler (c, y, s, lambda_, r, **kwargs) :\n",
    "    n = np.array([np.sum(c == j) for j in range(k)])\n",
    "    y_ = np.array([np.sum(y[c == j]) / nj for j, nj in enumerate(n)])\n",
    "    mean = (y_ * n * s + lambda_ * r) / (n * s + r)\n",
    "    var = 1 / (n * s + r)\n",
    "    sample = rng.randn(k)\n",
    "    return (mean + np.sqrt(var) * sample).tolist()\n",
    "\n",
    "def precisionSampler (c, y, mu, beta, omega, **kwargs) :\n",
    "    n = np.array([np.sum(c == j) for j in range(k)])\n",
    "    deltas = np.array([np.sum(np.linalg.norm(y[c == j] - mu_j)**2) for j, mu_j in enumerate(mu)])\n",
    "    shape = beta + n\n",
    "    mean = ((deltas + omega * beta) / shape) ** (-1)\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale).tolist()\n",
    "\n",
    "def lambdaSampler(mu, mu_y, var_y, r, **kwargs) :\n",
    "    mean = (mu_y*1/var_y + r*sum(mu))/(1/var_y + k*r)\n",
    "    var = 1/(1/var_y + k*r)\n",
    "    return mean + np.sqrt(var) * rng.randn()\n",
    "\n",
    "def rSampler (mu, mu_y, var_y, lambda_, **kwargs) :\n",
    "    shape = k + 1\n",
    "    mean = ((1/shape)*(var_y + np.linalg.norm(np.array(mu) - lambda_)**2))**-1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)\n",
    "\n",
    "def betaSampler (s, omega, **kwargs) :\n",
    "    s_ = np.array(s)\n",
    "    def logU (y) :\n",
    "        if k == 0 : \n",
    "            return y + (-3/2) * (y - math.log(2)) - (1/2) * torch.exp(-y)\n",
    "        beta = torch.exp(y)\n",
    "        t1 = (-k * torch.lgamma(beta/2))\n",
    "        t2 = (-1/(2*beta))\n",
    "        t3 = ((k*beta-3)/2)*(y - math.log(2))\n",
    "        t4 = (beta/2)*np.sum(np.log(s_*omega) - s_*omega)\n",
    "        return y + t1 + t2 + t3 + t4\n",
    "    sample = adaptiveRejectionSampling(logU, 1, (-math.inf, math.inf)).pop()\n",
    "    return np.exp(float(sample))\n",
    "\n",
    "def omegaSampler (s, var_y, beta, **kwargs) :\n",
    "    shape = k*beta + 1\n",
    "    mean = ((1/shape)*(var_y + beta * sum(s)))**-1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)\n",
    "\n",
    "def alphaSampler (c, **kwargs) :\n",
    "    n = len(c)\n",
    "    def logU (y) :\n",
    "        alpha = torch.exp(y)\n",
    "        t1 = y * (k - (3/2))\n",
    "        t2 = -1/(2 * alpha)\n",
    "        t3 = torch.lgamma(alpha)\n",
    "        t4 = -torch.lgamma(n + alpha)\n",
    "        return y + t1 + t2 + t3 + t4\n",
    "    sample = adaptiveRejectionSampling(logU, 1, (-math.inf, math.inf)).pop()\n",
    "    return np.exp(float(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:47<00:00,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'alpha': 2.7989905861484146,\n",
      "  'beta': 3.4397421232442187,\n",
      "  'c': array([19, 19, 19, ...,  3, 16, 10]),\n",
      "  'lambda_': 4.924026173661952,\n",
      "  'mu': [ -19.494551359831,\n",
      "          -0.7293464961791816,\n",
      "          0.4958198270348406,\n",
      "          21.031100267077274,\n",
      "          -0.01828486201131333,\n",
      "          0.3860443728996452,\n",
      "          -0.4970698425206666,\n",
      "          -0.12211920428072907,\n",
      "          20.061311343647514,\n",
      "          19.851764204714435,\n",
      "          20.613179638827372,\n",
      "          0.9137267094162124,\n",
      "          0.13925687800498474,\n",
      "          -0.1662342996669205,\n",
      "          0.745990543953103,\n",
      "          -1.052098125163707,\n",
      "          18.997005670121172,\n",
      "          -1.648330074466446,\n",
      "          19.498023676800734,\n",
      "          -20.480634185589327,\n",
      "          1.4493100936760097,\n",
      "          20.59928210067619],\n",
      "  'omega': 0.14062135563536726,\n",
      "  'r': 0.006901544910622089,\n",
      "  's': [ 1.3626752646959075,\n",
      "         2.0901701127437957,\n",
      "         6.595832287846267,\n",
      "         1.0897392989270382,\n",
      "         7.680518769471117,\n",
      "         4.489443369141746,\n",
      "         9.516238724122228,\n",
      "         3.468529136366775,\n",
      "         2.5751968404570933,\n",
      "         5.670366575477092,\n",
      "         1.95278659101096,\n",
      "         7.84670738232163,\n",
      "         2.853529291768139,\n",
      "         3.545855267157991,\n",
      "         1.0937954308782334,\n",
      "         10.498448574193565,\n",
      "         3.194688805743438,\n",
      "         2.771209097557866,\n",
      "         1.0361373193337233,\n",
      "         1.1264936654701028,\n",
      "         4.406129690062997,\n",
      "         6.676792008244468],\n",
      "  'unrepresentedMu': -3.954119420395344,\n",
      "  'unrepresentedPrecision': 8.641055030582866}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'alpha': 1,\n",
    "    'mu': [],\n",
    "    's': [],\n",
    "    'lambda_': mu_y,\n",
    "    'r': 1, \n",
    "    'beta': var_y,\n",
    "    'omega': var_y,\n",
    "    'c': np.ones(n) * -1,\n",
    "    'unrepresentedMu': 0,\n",
    "    'unrepresentedPrecision': 1\n",
    "}\n",
    "\n",
    "samplers = {\n",
    "    'alpha': alphaSampler,\n",
    "    'c': cSampler,\n",
    "    'mu': meanSampler,\n",
    "    's': precisionSampler,\n",
    "    'lambda_': lambdaSampler,\n",
    "    'r': rSampler, \n",
    "    'beta': betaSampler,\n",
    "    'omega': omegaSampler,\n",
    "    'unrepresentedMu': unrepresentedMeanSampler,\n",
    "    'unrepresentedPrecision': unrepresentedPrecisionSampler\n",
    "}\n",
    "\n",
    "others = {\n",
    "    'mu_y': mu_y,\n",
    "    'var_y': var_y,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "inferredParameters = gibbs(parameters, samplers, 200, **others)\n",
    "pprint.PrettyPrinter(indent=2).pprint(inferredParameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of classes are too many. But the mean and precisions are still accurate. Hence, if we were to sample from the model given by the inferred parameters, the samples will resemble those from the actual distribution!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
